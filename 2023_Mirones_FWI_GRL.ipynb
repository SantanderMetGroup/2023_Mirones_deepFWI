{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53940d8f",
   "metadata": {},
   "source": [
    "# Towards Spatio-temporally Consistent Multi-Site Fire Danger Downscaling with Explainable Deep Learning\n",
    "\n",
    "***Journal of Geophysical Research: Machine Learning and Computation***\n",
    "\n",
    "**O. Mirones, J. Baño-Medina, S. Brands, J. Bedia**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21aef33",
   "metadata": {},
   "source": [
    "This notebook reproduces the most important results presented in the manuscript entitled **Towards Spatio-temporally Consistent Multi-Site Fire Danger Downscaling with Explainable Deep Learning**, submitted to the journal *Journal of Geophysical Research: Machine Learning and Computation* by *O. Mirones, J. Baño-Medina, S.Brands, J. Bedia*. \n",
    "This paper analyzes the ability of state-of-the-art machine learning algorithms, especially Convolutional Long Short Term Memory (ConvLSTM) neural networks, to model the spatial structure of the Fire Weather Index (**FWI**)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d56cab",
   "metadata": {},
   "source": [
    "## Table of contents:\n",
    "*  [1 Preparing the R environment](#1-bullet)\n",
    "*  [2  Load data](#2-bullet)\n",
    "    *  [2.1  Load predictor data](#2.1-bullet)\n",
    "    *  [2.2  Load predictand data](#2.2-bullet)\n",
    "*  [3  Downscaling: Cross-validation](#3-bullet)\n",
    "    *  [3.1  Benchmark methods](#3.1-bullet)\n",
    "        *  [3.1.1  Generalized Linear Models](#3.1.1-bullet)\n",
    "        *  [3.1.2  Convolutional Neural Network Gaussian](#3.1.2-bullet)\n",
    "    *  [3.2  Convolutional Neural Network Multi-Gaussian](#3.2-bullet)\n",
    "    *  [3.3 Convolutional LSTM Network Multi-Gaussian](#3.3-bullet)\n",
    "*  [4  Results](#4-bullet)\n",
    "    *  [4.1  Correlograms](#4.1-bullet)\n",
    "    *  [4.2  Mutual Information](#4.2-bullet)\n",
    "    *  [4.3 Autocorrelation Function (ACF)](#4.3-bullet)\n",
    "    *  [4.4 Model explainability](#4.4-bullet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f421fff",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> This notebook was run on a machine with the following technical specifications:\n",
    "    \n",
    "\n",
    "- Operating system: Ubuntu 18.04.3 LTS (64 bits)\n",
    "- Memory: 60 GiB (some process may need upon 128 GiB!!!)\n",
    "- Processor: 2x Intel(R) Xeon(R) CPU E5-2670 0 @ 2.60GHz (16 cores, 32 threads)\n",
    "\n",
    "Some calculations could take several hours (even days)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf5cd79",
   "metadata": {},
   "source": [
    "## 1. Preparing the R environment <a class=\"anchor\" id=\"1-bullet\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112d48a6",
   "metadata": {},
   "source": [
    "In particular, the following C4R libraries are used along the notebook:\n",
    "\n",
    "\n",
    " * `loadeR` (v1.7.0) for data loading,\n",
    " * `transformeR` (v2.1.0) for data manipulation, \n",
    " * [`downscaleR`](https://doi.org/10.5194/gmd-13-1711-2020) (v3.3.2) for downscaling and\n",
    " * [`downscaleR.keras`](https://doi.org/10.5194/gmd-13-2109-2020) (v1.0.0) for downscaling with neural networks and"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cfdfae0",
   "metadata": {},
   "source": [
    "A frozen version of the above libraries and all the ones needed to reproduce the manuscript are installable through the `environment.yml` file using the `mamba` package, by executing the following commands in your command shell terminal: \n",
    "```shell\n",
    "$ mamba env create -n deep-fwi --file=environment.yml\n",
    "```\n",
    "\n",
    "Once installed, you activate the newly created `deep-fwi` environment using `conda` by typing the following command:\n",
    "```shell\n",
    "$ conda activate deep-fwi\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5c92dd",
   "metadata": {},
   "source": [
    "Once in the environment, type `R` and load the libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743b1de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Loading libraries\n",
    "library(loadeR)\n",
    "library(transformeR)\n",
    "library(downscaleR)\n",
    "library(magrittr)\n",
    "library(downscaleR.keras)\n",
    "library(tfprobability)\n",
    "library(MASS)\n",
    "library(VALUE)\n",
    "library(geosphere)\n",
    "library(scales)\n",
    "library(abind)\n",
    "library(grid)\n",
    "library(gridExtra)\n",
    "library(lattice)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2cab16",
   "metadata": {},
   "source": [
    "We load some auxiliary functions from the `utils` directory. The files `./utils/topologiesCNN.R` and `./utils/topologyConvLSTM.R` contain the code implementing the convolutional and the convolutional-LSTM (ConvLSTM) neural networks respectively used in this study, which are based on tensorflow-keras.\n",
    "\n",
    "+ `./utils/downscaleCV.keras.tfprobability.R` is a function enabling cross-validation over a convolutional neural network coded with the library tensorflow-probability.\n",
    "+ `./utils/downscale.convLSTM.R` is a function enabling cross-validation over a ConvLSTM neural network coded with the library tensorflow-probability.\n",
    "+ `./utils/dimFix.R` completes missing dimensions of VALUE objects. \n",
    "+ `./utils/corrMat.VALUE.R`computes the cross correlation matrices between stations that serve as input for plotting functions. `dimFix.R` and `corrMat.VALUE.R` files are a variation of its namesake from the package `VALUE`, adapted for use with notebook data structures. \n",
    "+ `./utils/corrL_allST.R` returns the correlation length for the `corrMat.VALUE` output. The `corrL_allST` function uses the file `./utils/corr_length.R` to calculate the correlation length based on the smoothed fit to the given data.\n",
    "+ `./utils/smooth_corr.R` smooths using a loess filter with standard settings.\n",
    "+ `./utils/miMat.VALUE.R`computes the mutual information between stations. This file is a version of ist namesake from the package `VALUE`, adapted for the Fire Weather Index (`FWI`). \n",
    "+ `./utils/ascii_export.R`, is designed to save the predictions obtained in ASCII format, ensuring compatibility with various text editors and data processing tools.\n",
    "+  `./utils/station_table.csv` contains basic characteristics of the stations studied in this work. This information is particularly useful for plotting the explainable saliency maps for each climatic region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67eb2009",
   "metadata": {},
   "outputs": [],
   "source": [
    "source(\"./utils/topologiesCNN.R\")\n",
    "source(\"./utils/topologyConvLSTM.R\")\n",
    "source(\"./utils/downscaleCV.keras.tfprobability.R\")\n",
    "source(\"./utils/downscale.convLSTM.R\")\n",
    "source(\"./utils/dimFix.R\")\n",
    "source(\"./utils/corrMat.VALUE.R\")\n",
    "source(\"./utils/corrL_allST.R\")\n",
    "source(\"./utils/corr_length.R\")\n",
    "source(\"./utils/smooth_corr.R\")\n",
    "source(\"./utils/miMat.VALUE.R\")\n",
    "source(\"./utils//ascii_export.R\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2201143",
   "metadata": {},
   "source": [
    "Below we define parameters related to the cross-validation procedure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3215592",
   "metadata": {},
   "outputs": [],
   "source": [
    "folds <- list(1985:1991, 1992:1998, 1999:2004, 2005:2011)\n",
    "vars <- c(\"hus850\", \"ta850\", \"tas\", \"ua850\", \"va850\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a23573c",
   "metadata": {},
   "source": [
    "## 2 Load data <a class=\"anchor\" id=\"2-bullet\"></a>\n",
    "In this section we illustrate how to load the data into our `R` environment. Previously, we need to download the [data from the Zenodo repository](https://doi.org/10.5281/zenodo.8381438) and store it in our working directory. For the notebook to work succesfully, it is important to preserve the structure of directories encountered in Zenodo. In addition we create a new folder where we will store the estimated fields named `./data/predictions/`. The folder `figures` is created to store the figures of the results obtained. \n",
    "\n",
    "**Note**: The user must keep the directory structure of the downloaded data for the correct reproducibiliy of the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140abc32",
   "metadata": {},
   "outputs": [],
   "source": [
    "destfile <- tempfile()\n",
    "download.file(url = \"https://zenodo.org/records/8381438/files/data.zip?download=1\",\n",
    "              destfile = destfile)\n",
    "unzip(destfile, exdir = \".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c6d2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Store in \"./data/predictions\" the output data for the CNN models and predictand\n",
    "### and the figures produced in section 4 in \"./figures/\" .\n",
    "if (!dir.exists(\"./data/predictions/\")) dir.create(\"./data/predictions/\", showWarnings = FALSE)\n",
    "###\n",
    "if (!dir.exists(\"./figures/\")) dir.create(\"./figures/\", showWarnings = FALSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1889c01",
   "metadata": {},
   "source": [
    "## 2.1 Load predictor data <a class=\"anchor\" id=\"2.1-bullet\"></a>\n",
    "We load the following predictor variables: specific humidity at 850hPa (`hus850`), air temperature at 850 hPa (`ta850`), air surface temperature (`tas`), zonal wind velocity at 850 hPa (`ua850`), and meridional wind velocity at 850 hPa (`va850`). These variables are stored as `.nc` files in the directory `./data/predictors/`. We use `loadGridData` from library `loadeR` to load the data into our `R` session. Then we call function `makeMultiGrid` to bind these predictor variables into a single object named `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18541099",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Loading predictor data\n",
    "x <- lapply(vars, FUN = function(var) {\n",
    "  out <- loadGridData(paste0(\"./data/predictors/\", var, \".nc\"), var = var)\n",
    "  print(sprintf(\"Variable: %s loaded!\", var))\n",
    "  return(out)\n",
    "}) %>% makeMultiGrid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a90b49d",
   "metadata": {},
   "source": [
    "## 2.2 Load predictand data <a class=\"anchor\" id=\"2.1-bullet\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24f2134",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Loading predictand data from Zotero database\n",
    "ds <- \"./data/predictands\"\n",
    "di <- dataInventory(ds)\n",
    "str(di)\n",
    "\n",
    "y <- loadStationData(dataset = ds,\n",
    "                       var = \"fwi13\",\n",
    "                       lonLim = c(0,15),\n",
    "                       latLim = c(35,40),\n",
    "                       years = 1985:2011,\n",
    "                       tz = \"UTC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef1abb0",
   "metadata": {},
   "source": [
    "## 3 Downscaling: Cross-validation <a class=\"anchor\" id=\"3-bullet\"></a>\n",
    "In this section we perform cross-validation over the machine learning methods used in this study: analogs, generalized linear models, and three different convolutional neural network architectures. Therefore, we split the data into the following 4 chronological folds: 1985-1991, 1992-1998, 1999-2004, 2005-2011. We store the resulting predictions into the previously created `./data/predictions/` folder. These files will be used in the next section. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3acdf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.dir <- \"./data/predictions/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3235c2",
   "metadata": {},
   "source": [
    "The results presented in this section are predominantly based on libraries `downscaleR`, `transformeR` and `downscaleR.keras`. In particular we use the function `downscaleCV` from library `downscaleR` to perform cross-validation for the analogs and generalized linear models. Similary, we use the function `downscaleCV.keras` from `downscaleR.keras` to cross-validate the convolutional neural networks. The only exception is the convolutional model multi-site multi-gaussian which builds on function `downscaleCV.keras.tfprobability` located in the `utils` directory.\n",
    "\n",
    "The predictions are stored in an ASCII text file through the utilization of the `./utils/ascii_export.R` script. This function takes several inputs, including the predictions themselves, denoted as `pred`, the desired file name, represented by `filename`, and the directory where the predictions are to be stored, indicated as `dest.dir`. Additionally, it requires the specification of reference stations, which should correspond to the information found in the `./data/predictands/ascii_dataset/stations.txt` file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc22c4e5-ee5b-4da4-98e8-ad5bd8689ab1",
   "metadata": {},
   "source": [
    "## 3.1 Benchmark methods <a class=\"anchor\" id=\"3.1-bullet\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91864bf8",
   "metadata": {},
   "source": [
    "## 3.1.1 Generalized Linear Model <a class=\"anchor\" id=\"3.1.1-bullet\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64125067",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Downscaling generalized linear model\n",
    "number_of_neighbours = 16\n",
    "p <- downscaleCV(x = x,\n",
    "                 y = y,\n",
    "                 method = \"GLM\",\n",
    "                 family = \"gaussian\",\n",
    "                 prepareData.args = list(\"local.predictors\" = list(n = number_of_neighbours, vars = getVarNames(x))),\n",
    "                 scaleGrid.args = list(type = \"standardize\"),\n",
    "                 folds = folds)\n",
    "\n",
    "ascii_export(pred = p, filename = \"FWI-GLM\", \n",
    "             stations = \"./data/predictands/stations.txt\", \n",
    "             dest.dir = pred.dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ceff7b9",
   "metadata": {},
   "source": [
    "## 3.1.2 Convolutional Neural Network Gaussian <a class=\"anchor\" id=\"3.1.2-bullet\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a425f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Downscaling convolutional neural network multi-site gaussian\n",
    "model <- cnn_model(topology = \"cnn-gaussian\",\n",
    "                   input_shape = c(getShape(x, \"lat\"), getShape(x, \"lon\"), getShape(x, \"var\")),\n",
    "                   output_shape = getShape(y, \"loc\"),\n",
    "                   kernel_size = c(3,3),\n",
    "                   neurons = c(50, 50))\n",
    "p <- downscaleCV.keras(x = x,\n",
    "                       y = y,\n",
    "                       model = model,\n",
    "                       loss = \"gaussianLoss\",\n",
    "                       prepareData.keras.args = list(first.connection = \"conv\", last.connection = \"dense\", channels = \"last\"),\n",
    "                       compile.args = list(\"loss\" = gaussianLoss(last.connection = \"dense\"), \"optimizer\" = optimizer_adam(lr = 0.0001)),\n",
    "                       fit.args = list(\"batch_size\" = 100, \"epochs\" = 10000, \"validation_split\" = 0.1, \"verbose\" = 0, \"callbacks\" = list(callback_early_stopping(patience = 30))),\n",
    "                       scaleGrid.args = list(type = \"standardize\"),\n",
    "                       folds = folds)\n",
    "\n",
    "p <- computeTemperature(mean = subsetGrid(p, var = \"mean\"), log_var = subsetGrid(p, var = \"log_var\"))\n",
    "\n",
    "ascii_export(pred = p, filename = \"FWI-CNN-G\", \n",
    "             stations = \"./data/predictands/stations.txt\", \n",
    "             dest.dir = pred.dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038ec2be",
   "metadata": {},
   "source": [
    "## 3.2 Convolutional Neural Network Multi-Gaussian <a class=\"anchor\" id=\"3.2-bullet\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25292fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Downscaling convolutional neural network multi-site multi-gaussian\n",
    "negloglik <- function (x, rv_x) - (rv_x %>% tfd_log_prob(x))\n",
    "p <- downscaleCV.keras.tfprobability(x = x,\n",
    "                                     y = y,\n",
    "                                     samples = 1,\n",
    "                                     cnn_model.args = list(topology = \"cnn-multi-gaussian\",\n",
    "                                                           input_shape = c(getShape(x, \"lat\"), getShape(x, \"lon\"), getShape(x, \"var\")),\n",
    "                                                           output_shape = getShape(y, \"loc\"),\n",
    "                                                           kernel_size = c(3,3),\n",
    "                                                           neurons = c(200, 200)),\n",
    "                                     type_nn = \"multivariate-gaussian\",\n",
    "                                     prepareData.keras.args = list(first.connection = \"conv\", last.connection = \"dense\", channels = \"last\"),\n",
    "                                     compile.args = list(\"loss\" = negloglik, \"optimizer\" = optimizer_adam(lr = 0.0001)),\n",
    "                                     fit.args = list(\"batch_size\" = 100L, \"epochs\" = 10000L, \"validation_split\" = 0.1, \"verbose\" = 0L, \"callbacks\" = list(callback_early_stopping(patience = 30))),\n",
    "                                     scaleGrid.args = list(type = \"standardize\"),\n",
    "                                     folds = folds)\n",
    "\n",
    "ascii_export(pred = p, filename = \"FWI-CNN-MG\", \n",
    "             stations = \"./data/predictands/stations.txt\", \n",
    "             dest.dir = pred.dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093b0f13-8109-4c29-aa88-097c1d92d00d",
   "metadata": {},
   "source": [
    "## 3.3 Convolutional LSTM Network Multi-Gaussian <a class=\"anchor\" id=\"3.3-bullet\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a89ca7-c3bf-4ea4-8225-f8c778865e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Downscaling ConvLSTM neuroal network multi-site multi-gaussian\n",
    "time_steps <- 7\n",
    "p <- downscale.convLSTM(x = x, y = y,\n",
    "                       model.args = list(input_shape = c(time_steps, getShape(x, \"lat\"), getShape(x, \"lon\"), getShape(x, \"var\")),\n",
    "                                         output_shape = getShape(y, \"loc\"),\n",
    "                                         kernel_size = c(3,3),\n",
    "                                         neurons = c(200, 200)),\n",
    "                       sampling.strategy = \"kfold.chronological\",\n",
    "                       time_steps = 7,\n",
    "                       compile.args = list(\"loss\" = negloglik, \"optimizer\" = optimizer_adam(lr = 0.0001)),\n",
    "                       fit.args = list(\"batch_size\" = 100L, \"epochs\" = 10000L, \"validation_split\" = 0.1, \"verbose\" = 0L, \"callbacks\" = list(callback_early_stopping(patience = 30))),\n",
    "                       scaleGrid.args = list(type = \"standardize\"),\n",
    "                       folds = folds)\n",
    "\n",
    "ascii_export(pred = p, filename = \"FWI-ConvLSTM-MG\", \n",
    "             stations = \"./data/predictands/stations.txt\", \n",
    "             dest.dir = pred.dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca19c1d7",
   "metadata": {},
   "source": [
    "## 4 Results <a class=\"anchor\" id=\"4-bullet\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2a3f68",
   "metadata": {},
   "source": [
    "In this section, we will present an analysis of the results obtained from the different predictions. Firstly, we will display the correlograms that have been constructed for each prediction. Secondly, we will present the mutual information matrices. Lastly, we show the saliency maps, spatial representations of the feature importance in the model outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b13a4a",
   "metadata": {},
   "source": [
    "## 4.1 Correlograms <a class=\"anchor\" id=\"4.1-bullet\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55eb7cfa",
   "metadata": {},
   "source": [
    "Here, we assess the strength of linear relationships between observed and predicted `FWI` time series among different locations. We accomplish this by calculating pairwise *Spearman's* cross-correlations among all pairs of stations. To visualize these relationships, we construct a location plot by plotting the cross-correlation value for each station pair against their respective geographical distances.\n",
    "\n",
    "Next, we fit a local 2nd-order polynomial (loess) to obtain two curves—one for predictions and one for observations. This enables a straightforward visual assessment of the spatial correlation structure of predictions compared to the reference observations.\n",
    "\n",
    "For the analysis, we define the fire season as June to September (JJAS), and we set a threshold ($0.4$) for the correlation length and specify the correlation type. The correlations are then computed using the `corrL_allST` function.\n",
    "\n",
    "Additionally, we calculate the Root Mean Square Error (`RMSE`), Mean Absolute Error (`MAE`), and the correlation length bias (`CL bias`). These metrics provide valuable insights into the performance of the predictions compared to the reference observations. All the computed values, including correlations, correlation length, `RMSE`, `MAE`, and `CL bias`, are included in the correlation plots.\n",
    "\n",
    "To facilitate easy access and reference, the resulting figure will be stored in the `figures` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad510a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Defining season, threshold and correlation type\n",
    "\n",
    "sea <- \"JJAS\"\n",
    "tr <- .4\n",
    "corrtype <- \"spearman\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d36799",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Computing and plotting the correlogram for the reference observations\n",
    "pdf(\"./figures/correlograms.pdf\", width = 14, height = 8.5)\n",
    "par(mfrow = c(2,3))\n",
    "\n",
    "corrL_obs <- corrL_allST(data=y, type = \"after\", corrtype = corrtype,tr=tr,season=sea)\n",
    "\n",
    "plot(corrL_obs[['JJAS']], col = alpha('grey', 0.3), pch = 16, ylim = c(-0.3,1),\n",
    "     xlab = \"Distance [km]\", ylab = \"Correlation\", main = \"Obs\", cex.lab = 1.5, cex.main = 1.5, cex.axis = 1.5,\n",
    "     las = 1)\n",
    "abline(h = tr, lty = 2)\n",
    "abline(v = corrL_obs[[\"JJA\"]][[\"corrL\"]], lwd = 2, col = 'grey')\n",
    "lines(corrL_obs[[\"JJAS\"]][[\"x_fit\"]], corrL_obs[[\"JJAS\"]][[\"y_fit\"]], col = 'grey', lwd = 2.5)\n",
    "grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c6f093",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Computing and plotting the correlograms for the predictions\n",
    "preds <- c(\"FWI-GLM\", \"FWI-CNN-G\", \"FWI-CNN-MG\", \"FWI-ConvLSTM-MG\")\n",
    "\n",
    "\n",
    "for (i in c(1:length(preds))) {\n",
    "  \n",
    "  ###Loading prediction and computing the correlation length\n",
    "  \n",
    "  p <- loadStationData(\"./data/predictions/\",var = preds[i])\n",
    "  corrL_pred <- corrL_allST(data=p, type = \"after\", corrtype = corrtype,tr=tr,season=sea)\n",
    "  \n",
    "  ###Computing RMSE and MAE\n",
    "  \n",
    "  d <- corrL_obs[[\"JJAS\"]][[\"y\"]] - corrL_pred[[\"JJAS\"]][[\"y\"]]\n",
    "  mae <- mean(abs(d))\n",
    "  mse <- mean((d)^2)\n",
    "  rmse <- sqrt(mse)\n",
    "  bias.corrL <- corrL_obs[[\"JJAS\"]][[\"corrL\"]]-corrL_pred[[\"JJAS\"]][[\"corrL\"]]\n",
    "  \n",
    "  ###Visualization\n",
    "  plot(corrL_pred[['JJAS']], col = alpha('red', 0.3), pch = 16, ylim = c(-0.3,1),\n",
    "       xlab = \"Distance [km]\", ylab = \"Correlation\",\n",
    "       main = sub(\"FWI-\", \"\", preds[i]), \n",
    "       cex.lab = 1.5, cex.main = 1.5, cex.axis = 1.5, las = 1)\n",
    "  abline(h = tr, lty = 2)\n",
    "  abline(v = corrL_obs[[\"JJAS\"]][[\"corrL\"]], lwd = 2, col = 'grey')\n",
    "    \n",
    "  lines(corrL_obs[[\"JJAS\"]][[\"x_fit\"]], corrL_obs[[\"JJAS\"]][[\"y_fit\"]], col = 'grey', lwd = 2.5) \n",
    "  abline(v = corrL_pred[[\"JJAS\"]][[\"corrL\"]], lwd = 2, col = 'red')\n",
    "  lines(corrL_pred[[\"JJAS\"]][[\"x_fit\"]], corrL_pred[[\"JJAS\"]][[\"y_fit\"]], col = 'red', lwd = 2.5)\n",
    "    \n",
    "  text(1225,0.85, paste0(\"RMSE: \", round(rmse,3)), cex = 1.45)\n",
    "  text(1225,0.75, paste0(\"MAE: \", round(mae,3)), cex = 1.45)\n",
    "  text(1225,0.95, paste0(\"CL bias: \", round(bias.corrL, 3)), cex = 1.45)\n",
    "  grid()\n",
    " \n",
    "}\n",
    "\n",
    "dev.off()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8986b1",
   "metadata": {},
   "source": [
    "## 4.2 Mutual Information <a class=\"anchor\" id=\"4.2-bullet\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223a68b5",
   "metadata": {},
   "source": [
    "We proceed to plot each mutual information (`MI`) value, denoted as $M_{i,j}$, against the distance between the corresponding locations, i.e., $i$ and $j$. To capture the underlying patterns, we fit a second-degree loess curve to these plots.\n",
    "\n",
    "To determine the mutual information lengths (`MIL`), we establish MI thresholds for both the observations and the different downscaling methods. In this analysis, we utilize a MI threshold of $0.05$. This threshold allows for comparable results to correlation length analyses and aids in the identification of potential new information regarding the performance of each method.\n",
    "\n",
    "Based on the climatic regions, the stations are grouped together in a matrix. This matrix exhibits three primary groupings, each with a quadrilateral shape. These groupings correspond to the following climatic regions: the *Atlantic* region, the *Continental Mediterranean* region, and the *Coastal Mediterranean* stations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7d2e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Defining the order of the stations\n",
    "\n",
    "y[[\"Metadata\"]][[\"name\"]] <- c(\"REUS\",\"S.COMPOSTELA\",\"VIGO\",\"SORIA\",\"VALLADOLID\",\"ZAMORA\",\"LEON\",\"SALAMANCA\",\n",
    "                               \"MADRID-BARAJAS\",\"MADRID-RETIRO\",\"CIUDAD REAL\",\"BADAJOZ\",\"GRANADA\",\n",
    "                               \"SEVILLA\",\"MORON\",\"JEREZ\",\"ALMERIA\",\"MURCIA\",\"ALICANTE-ALTET\",\n",
    "                               \"ALICANTE\",\"CUENCA\",\"VALENCIA-AER.\",\"VALENCIA\",\"LOGRONO\",\"DAROCA\",\"TORTOSA\",\n",
    "                               \"MALLORCA\",\"MENORCA\",\"IBIZA\")\n",
    "\n",
    "###Defining the FWI90 event for the mutual information\n",
    "\n",
    "prob <- .9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c982aef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "###defining the color palettes\n",
    "mi.colors <- RColorBrewer::brewer.pal(11, \"Reds\") %>% colorRampPalette()\n",
    "spec.colors <- RColorBrewer::brewer.pal(11, \"Spectral\") %>% colorRampPalette()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44f59f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#modification of stations order\n",
    "station.ids <- y[[\"Metadata\"]][[\"station_id\"]][c(2:10,21,24,13,12,11,25,1,27:29,18:20,22,23,26,14:17)]\n",
    "y.sts <- subsetGrid(y.sts, station.id = station.ids) %>% redim(member = FALSE, loc = TRUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50bd697d",
   "metadata": {},
   "source": [
    "In this step, we compute the `MI` matrix for both the reference observations and the predictions. To perform these computations, we utilize the `miMat.VALUE` function. This function enables us to calculate the `MI` values and construct the `MI` matrix, which represents the pairwise `MI` between different locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c25d534",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Mutual Information matrix computation for the observations\n",
    "\n",
    "mi.matrix.obs <- miMat.VALUE(y.sts,\n",
    "                         predictionObj = NULL,\n",
    "                         season = \"JJAS\",\n",
    "                         threshold = NULL,\n",
    "                         prob = prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c74ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Mutual Information matrix computation for the predictions\n",
    "mi.matrix.list <- list()\n",
    "preds <- c(\"FWI-GLM\", \"FWI-CNN-G\",\"FWI-ConvLSTM-MG\")\n",
    "preds.names <- c(\"GLM\",\"CNN-G\", \"ConvLSTM-MG\")\n",
    "\n",
    "for (i in c(1:length(preds))) {\n",
    "\n",
    "  ### Loading predictions\n",
    "    \n",
    "  p <- loadStationData(\"./data/predictions/\",var = preds[i])\n",
    "  \n",
    "  ###Ordering stations by location\n",
    "  \n",
    "  p <- subsetGrid(p, station.id = station.ids) %>% redim(member = FALSE, loc = TRUE)\n",
    "  \n",
    "  ###Computing the Mutual Information matrix\n",
    "  \n",
    "  mi.matrix.list[[i]] <- miMat.VALUE(p,\n",
    "                predictionObj = NULL,\n",
    "                season = \"JJAS\",\n",
    "                threshold = NULL,\n",
    "                prob = prob)\n",
    "  \n",
    "  \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66787ea",
   "metadata": {},
   "source": [
    "Lastly, we proceed to plot the `MI` matrices for the `FWI90` event during the fire season (June to September). The first matrix computed represents the MI for the reference observations, displayed in the upper triangle, and the best-performing model (`CNN-MG`) in the lower triangle. The bottom left and right matrices depict the `MI` bias of the methods compared to the observations.\n",
    "\n",
    "In the `MI` bias matrices, the analogs method is excluded from the representation since `MI` biases are negligible by construction. These matrices are divided to show the bias corresponding to the different methods in the upper and lower triangles. Only pairs of stations with `MI` values $\\ge 0.05$ for the observations are displayed in the `MI` bias matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b34517d",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Setting some atributes for the figure\n",
    "scales.list <- list(x = list(labels = attributes(mi.matrix.obs[[1]])[['station_names']], rot = 90,\n",
    "                             at = seq(1,ncol(mi.matrix.obs[[1]]),1), cex = .65),\n",
    "                    y = list(labels = attributes(mi.matrix.obs[[1]])[['station_names']],\n",
    "                             at = seq(1,ncol(mi.matrix.obs[[1]]),1), cex = .65),\n",
    "                   alternating = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56805d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Computing the first matrix: the upper diagonal contains the mutual information for the obs;\n",
    "###the lower diagonal is for the cnn-multi-site-multi-gaussian\n",
    "\n",
    "mi.matrix <- mi.matrix.obs\n",
    "\n",
    "idx.lower.tri <- which(lower.tri(mi.matrix[[\"JJAS\"]]), arr.ind = T)\n",
    "\n",
    "###replacing the lower triangle matrix\n",
    "mi.matrix[[\"JJAS\"]][idx.lower.tri] <- mi.matrix.list[[grep(\"ConvLSTM-MG\", preds)]][[\"JJAS\"]][idx.lower.tri] \n",
    "\n",
    "l1 <- levelplot(mi.matrix[['JJAS']], ylab = \"AEMET_13UTC_FWI13 (Obs)\" ,\n",
    "                xlab = \"ConvLSTM-MG\",\n",
    "                scales = scales.list, main = \"\", col.regions = mi.colors(100),\n",
    "               colorkey = list(space = \"bottom\", height = 1, width = 1)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410efd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Replacing with NAs those values with MI < 0.05 (below the threshold defined)\n",
    "\n",
    "idx.na <- which(mi.matrix.obs[['JJAS']] < 0.05, arr.ind = T)\n",
    "\n",
    "i <- 1   \n",
    "mi.matrix <- mi.matrix.list[[i]]\n",
    "idx.lower.tri <- which(lower.tri(mi.matrix[[\"JJAS\"]]), arr.ind = T)\n",
    "mi.matrix[[\"JJAS\"]][idx.lower.tri] <- mi.matrix.list[[i+1]][[\"JJAS\"]][idx.lower.tri]\n",
    "  \n",
    "mi.matrix[[\"JJAS\"]][idx.na] <- NA\n",
    "  \n",
    "l2 <- levelplot(mi.matrix[[\"JJAS\"]]-mi.matrix.obs[[\"JJAS\"]], ylab = sub(\"FWI-\", \"\", preds[+1]),\n",
    "                 xlab = sub(\"FWI-\", \"\", preds[i+1]),scales = scales.list, main = \"\",\n",
    "                 col.regions = rev(spec.colors(100)), at = seq(-0.15,0.15,0.01),\n",
    "                 colorkey = list(space = \"bottom\", height = 1, width = 1)) \n",
    "\n",
    "pdf(\"./figures/MI_Matrices.pdf\", width = 22.5, height = 11)\n",
    "\n",
    "grid.arrange(l1, l2, nrow = 1, ncol = 2)\n",
    "\n",
    "dev.off()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8f8044-de67-42ea-bc07-71d016a0ae5a",
   "metadata": {},
   "source": [
    "## 4.3 Autocorrelation Function (ACF) <a class=\"anchor\" id=\"4.3-bullet\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fef2d8-7662-47f0-9790-bb017d3727e5",
   "metadata": {},
   "source": [
    "The temporal characteristics of the different models are analyzed using the `ACF` up to a `365` day lag, offering insights into the models' ability to capture temporal dependencies over a one-year period. The ACF results for three representative locations --`Santiago de Compostela-Labacolla (ATL)`, `Cuenca (CONTM)`, and `Jerez de la Frontera-Aeropuerto (COASM)`-- are computed. Each panel illustrates the `ACF` and `ACF bias` for both the observational data and the three deep learning approaches: `CNN-G`, `CNN-MG`, and `ConvLSTM-MG`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54560cf-6143-44cb-970e-6cd5de858fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "st.index <- c(2,16,21)\n",
    "lstm <- loadStationData(\"./data/predictions/\",var = preds[4])\n",
    "cnn_mg <- loadStationData(\"./data/predictions/\",var = preds[3])\n",
    "cnn_g <- loadStationData(\"./data/predictions/\",var = preds[2])\n",
    "\n",
    "y <- intersectGrid(y, lstm, which.return = 1)\n",
    "\n",
    "pdf(\"./figures/acf_figure.pdf\", width = 13, height = 10)\n",
    "par(mfrow = c(2,2))\n",
    "for (i in st.index) {\n",
    "\n",
    "  lstm_acf <- acf(lstm[['Data']][,i], plot = F, lag.max = 365)\n",
    "  cnn_mg_acf <- acf(cnn_mg[['Data']][,i], plot = F, lag.max = 365)\n",
    "  y_acf <- acf(y[['Data']][,i], plot = F, lag.max = 365)\n",
    "  cnn_g_acf <- acf(cnn_g[['Data']][,i], plot = F, lag.max = 365)\n",
    "  \n",
    "  plot(y_acf[['Data']], type = 'l', col = 'black', lwd = 2, main = y[['Metadata']][['name']][i], ylab = \"Observed ACF\", xlab = \"Lags\",\n",
    "       cex.lab = 1.5, \n",
    "       cex.axis = 1.5)\n",
    "  lines(cnn_mg_acf[['acf']], col = 'red', lwd = 2)\n",
    "  lines(lstm_acf[['acf']], col = 'blue', lwd = 2)\n",
    "  lines(cnn_g_acf[['acf']], col = 'green', lwd = 2)\n",
    "  abline(h = 0)\n",
    "\n",
    "  par(new = TRUE)\n",
    "  plot((lstm_acf[['acf']] - y_acf[['acf']])[-1], type = 'l', col = 'blue', lwd = 1,axes = FALSE, xlab = \"\", ylab = \"\", \n",
    "       ylim = c(-0.5,0.2), lty = 'dashed', cex.lab = 1.5, \n",
    "       cex.axis = 1.5)\n",
    "  lines((cnn_mg_acf[['acf']] - y_acf[['acf']])[-1], type = 'l', col = 'red', lwd = 1, lty = 'dashed')\n",
    "  lines((cnn_g_acf[['acf']] - y_acf[['acf']])[-1], type = 'l', col = 'green', lwd = 1, lty = 'dashed')\n",
    "  abline(h = 0, lty = 'dashed')\n",
    "  axis(side = 4, cex.axis = 1.5)\n",
    "  legend(\"topright\",legend = c(\"Observation\",\"CNN-G\",\"CNN-MG\",\"convLSTM-MG\",\"CNN-G Bias\",\"CNN-MG Bias\",\"convLSTM-MG Bias\"), \n",
    "         col = c(\"black\",\"green\",\"red\",\"blue\",\"green\", \"red\", \"blue\"), lty = c(1,1,1,1,2,2,2,2), cex = 1.5)\n",
    "  grid()\n",
    "}\n",
    "dev.off()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abe558c-5fe1-41d8-b89f-28db7f2fad18",
   "metadata": {},
   "source": [
    "## 4.4 Model explainability <a class=\"anchor\" id=\"4.5-bullet\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b835cf-16b7-4ccb-a040-a73660712eff",
   "metadata": {},
   "source": [
    "For this section, first we must modify the `x` and `y` variables to adequate the input to the ConvLSTM model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beaeb6d9-d5f9-4695-a094-828aa8b08944",
   "metadata": {},
   "outputs": [],
   "source": [
    "x <- getTemporalIntersection(x,y,which.return = \"obs\")\n",
    "y <- getTemporalIntersection(x,y,which.return = \"prd\")\n",
    "  \n",
    "x.aux <- x[['Data']]  # Dimensions:(removing member dimension): (var, time, lat, lon)\n",
    "y.aux <- y[['Data']]  # Dimensions: (time,loc)\n",
    "\n",
    "#reorder dimensions: (time_steps, lat, lon, vars)\n",
    "x.aux <- aperm(x.aux, c(2, 3, 4, 1))\n",
    "#create sequences with time steps\n",
    "\n",
    "num_samples <- dim(x.aux)[1] - time_steps + 1\n",
    "x_seq <- array(0, dim = c(num_samples, time_steps, dim(x.aux)[2], dim(x.aux)[3], dim(x.aux)[4]))\n",
    "y_seq <- array(0, dim = c(num_samples, dim(y.aux)[2]))\n",
    "    \n",
    "for (i in 1:num_samples) {\n",
    "    x_seq[i, , , , ] <- x.aux[i:(i + time_steps - 1), , , ]\n",
    "    y_seq[i, ] <- y.aux[i + time_steps - 1, ]  # Predict last step of the sequence\n",
    "}\n",
    "    \n",
    "x <- subsetDimension(x, dimension = \"time\", indices = time_steps:getShape(x, \"time\"))\n",
    "y <- subsetDimension(y, dimension = \"time\", indices = time_steps:getShape(y, \"time\"))\n",
    "  \n",
    "x[['Data']] <- x_seq\n",
    "y[['Data']] <- y_seq\n",
    "  \n",
    "attr(x[['Data']], \"dimensions\") <- c(\"time\", \"member\", \"lat\", \"lon\", \"var\") #add time_steps as member dimension\n",
    "attr(y[['Data']], \"dimensions\") <- c(\"time\", \"loc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9eee518-dbd6-403e-8d5b-63cb322e019d",
   "metadata": {},
   "source": [
    "Now, we present the aggregated saliency maps for the most representative predictor variables during FWI90 events across the COASM climatic regions. The mean FWI saliency maps exhibit similar spatial distributions, with the main differences being in the intensity of feature significance and the comparable contributions from various variables. As a result, the FWI90 saliency maps are considered more suitable for analyzing the contributions of predictors to model construction, particularly in the context of the most impactful wildfire events.\n",
    "\n",
    "Specifically, we focus on \"saliency maps\" (SM), which are spatial representations of feature importance in the model outputs. The relevance of each variable is assessed primarily by calculating the gradients of the output with respect to the input. These gradients are then back-propagated through the hidden layers of the network and visualized as saliency maps. Various techniques exist for computing this variable relevance; in this work, we use the Integrated Gradients (IG) method. For further details, please refer to the paper attached to this notebook.\n",
    "\n",
    "In line with previous research, we post-process the \"raw\" output of the XAI technique by following these steps:\n",
    "+ **`Step 1`**: We first compute the absolute value since the interest lies in the value regardless of its sign.\n",
    "+ **`Step 2`**: To avoid gradient shattering, we filter out the values above a threshold of 1.5E-3. The percentage for each saliency map is individually calculated by dividing the value of each feature by the total sum of all features.\n",
    "+ **`Step 3`**: Compute the percentage of each saliency map. Lastly, the saliency maps are aggregated by averaging the values over the training period, resulting in a collection of maps with the same dimensions as the input features, each representing a predictor variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8c3dc7-7769-487b-9a79-08cbedfa2b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_intgra <- lapply(1:getShape(y, \"loc\"), FUN = function(ind_site) {\n",
    "\n",
    "  y <- y[['Data']][, ind_site, drop = F]\n",
    "  #dimensions(time, time_steps, lat, lon, var)\n",
    "  input <- x[['Data']]\n",
    "  #Computing integrated gradients\n",
    "  batch <- 200\n",
    "  samples <- dim(input)[1]\n",
    "  init_batches <- seq(1, samples, batch)\n",
    "  end_batches <- c(seq(batch, samples, batch), samples) %>% unique()\n",
    "  saliency_array <- lapply(1:length(init_batches), FUN = function(ind_batch) {\n",
    "    init_batch <- init_batches[ind_batch]\n",
    "    end_batch <- end_batches[ind_batch]\n",
    "    print(sprintf(\"Batch %i/%i\", which(init_batch == init_batches), length(init_batches)))\n",
    "    input_to_compute_intgra <- input[init_batch:end_batch,,,,]\n",
    "     \n",
    "    predictor_dims <- dim(input)\n",
    "    baseline <- baseline_array <- array(0, dim = predictor_dims)\n",
    "\n",
    "    num_steps <- 50\n",
    "    \n",
    "    # 1. Do interpolation.\n",
    "    interpolated_input <- list()\n",
    "    for (step in 1:num_steps) interpolated_input[[step]] <- baseline + (step / num_steps) * (input - baseline)\n",
    "    \n",
    "    \n",
    "    # 2. Get the gradients\n",
    "    grads = list()\n",
    "    for (index_int_input in 1:length(interpolated_input))  {\n",
    "                                             model = model)\n",
    "      input <- tf$cast(input, tf$float32)\n",
    "      \n",
    "      site <- ind_site\n",
    "      \n",
    "      with(tf$GradientTape() %as% tape, {\n",
    "        tape$watch(input)\n",
    "        preds <- model(input) %>% tfd_sample()\n",
    "        preds <- preds[,site]\n",
    "      })\n",
    "      \n",
    "      grads <- tape$gradient(preds, input)\n",
    "    }\n",
    "    grads <- tf$convert_to_tensor(grads, dtype = tf$float32)\n",
    "    \n",
    "    # 3. Approximate the integral using the trapezoidal rule\n",
    "    grads = (grads[1:(num_steps-1),,,,] + grads[2:num_steps,,,,]) / 2.0  ### generalizar esto para inputs que no sean 4D!!!\n",
    "    avg_grads = tf$reduce_mean(grads, axis = 0L)\n",
    "    \n",
    "    # 4. Calculate integrated gradients and return\n",
    "    integrated_grads <- (input - baseline) * avg_grads\n",
    "    return(integrated_grads %>% as.array())\n",
    "  }) %>% abind::abind(along = 1)\n",
    "  \n",
    "  \n",
    "  #Free memory\n",
    "  gc()\n",
    "  \n",
    "  \n",
    "  ### Computing percentage and post-processing:\n",
    "  ##Step 1: take the absolute value\n",
    "  saliency_array %<>% abs()\n",
    "  ##Step 2 and 3: filter the lowest values and compute saliency maps by dividing the value by the total sum of each feature\n",
    "  filter <- 0.0015\n",
    "  saliency_array[saliency_array < filter] <- 0\n",
    "  for (i in 1:dim(saliency_array)[1]) {\n",
    "    saliency_array[i,,,,] <- saliency_array[i,,,,] / sum(saliency_array[i,,,,]) \n",
    "  }\n",
    "  \n",
    "  \n",
    "  #Insert into a c4r object\n",
    "  template <- subsetDimension(x, dimension = \"time\", indices = time_steps:getShape(x, \"time\")) %>% redim(drop = T)\n",
    "  template[[\"Data\"]] <- aperm(saliency_array, c(5,2,1,3,4))\n",
    "  attr(template[[\"Data\"]], \"dimensions\") <- c(\"var\", \"time_step\",\"time\", \"lat\", \"lon\")\n",
    "  \n",
    "  return(template)\n",
    "\n",
    "}) #list where each element is a station \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37fb678-4654-47a7-bac5-64d36c505a8b",
   "metadata": {},
   "source": [
    "Finally, we plot the aggregated saliency maps for the most representative predictor variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a07b8d1-b52a-497a-b613-508310e8f9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "regions <- c(\"COASM\",\"ATL\",\"CONTM\")\n",
    "figs.region <- list()\n",
    "stations <- read.csv(\"./utils/station_table.csv\")\n",
    "\n",
    "cb <- brewer.pal(n = 8, \"OrRd\")\n",
    "cb <- c(\"#FFFFFF\", cb) %>% colorRampPalette()\n",
    "names.ts <- c(\"time step = t-6\",\"time step = t-5\",\"time step = t-4\",\n",
    "              \"time step = t-3\",\"time step = t-2\",\"time step = t-1\",\n",
    "              \"time step = t\")\n",
    "figs.region <- list()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55bd935-8f9f-4f27-8bd2-bd416b8a8489",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_intgra <- grid_intgra\n",
    "\n",
    "for(ind_region in 1:length(regions)) {\n",
    "  region <- regions[ind_region]\n",
    "  print(region)\n",
    "  if (region == \"COASM\") {ind_station.ids <- which(stations[[\"climatic_region\"]] == \"COASM\")}\n",
    "  if (region == \"ATL\") {ind_station.ids <- which(stations[[\"climatic_region\"]] == \"ATL\")}\n",
    "  if (region == \"CONTM\") {ind_station.ids <- which(stations[[\"climatic_region\"]] == \"CONTM\")}\n",
    "  \n",
    "  station.ids <- y[[\"Metadata\"]][[\"station_id\"]][ind_station.ids]\n",
    "  y.st <- subsetGrid(y, station.id = station.ids) %>% filterNA()\n",
    "  coords_region <- y.st[[\"xyCoords\"]]\n",
    "  \n",
    "  y.st <- intersectGrid(y.st, lstm_intgra[ind_station.ids][[1]], which.return = 1)\n",
    "  idx.fwi90 <- which(y.st[[\"Data\"]] >= quantile(y.st[[\"Data\"]], .9, na.rm = T), arr.ind = T)\n",
    "  idx.fwi90 <- unique(idx.fwi90[,1])\n",
    "  \n",
    "  ###fwi90 convLSTM saliency maps\n",
    "  grid_intgra <- lstm_intgra[ind_station.ids]\n",
    "  for (j in 1:length(grid_intgra)) {\n",
    "    grid_intgra[[j]] <- intersectGrid(grid_intgra[[j]], y.st, which.return=1)\n",
    "    grid_intgra[[j]] <- subsetDimension(grid_intgra[[j]], dimension = \"time\", indices = idx.fwi90)\n",
    "  }\n",
    "\n",
    "  sum.intgra <- array(0, dim = getShape(grid_intgra[[1]]))\n",
    "  for (j in 1:length(grid_intgra)) {\n",
    "    sum.intgra <- sum.intgra + grid_intgra[[j]][['Data']]\n",
    "  }\n",
    "  sum.intgra <- sum.intgra/length(grid_intgra)\n",
    "  grid_aux <- grid_intgra[[1]]\n",
    "  grid_aux[['Data']] <- sum.intgra\n",
    "  #change time_step dimension by member to enable climate4R libraries\n",
    "  attr(grid_aux[['Data']], \"dimensions\") <- c(\"var\",\"member\", \"time\",\"lat\",\"lon\")\n",
    "  grid_aux <- climatology(grid_aux)%>% gridArithmetics(100, operator = \"*\") %>% redim(drop = T)\n",
    "  \n",
    "  sp.layout.args <- list(list(sp::SpatialPoints(coords_region),\n",
    "                                first = FALSE,\n",
    "                                col = \"black\",\n",
    "                                pch = 16))\n",
    "    \n",
    "  ###figures convLSTM\n",
    "  at <- seq(0,round(max(grid_aux[['Data']]), 2),round(max(grid_aux[['Data']]), 2)/20)\n",
    "  \n",
    "    \n",
    "  figs_fwi <- lapply(getVarNames(grid_aux), FUN = function(var) {\n",
    "    idx.var <- which(getVarNames(grid_aux) == var)\n",
    "    \n",
    "    if(var == \"hus850\"){\n",
    "      #at and cb values depending on the variable\n",
    "      cb.new <- brewer.pal(n = 8, \"Blues\")\n",
    "      cb.new <- c(\"#FFFFFF\", cb.new) %>% colorRampPalette()\n",
    "      grid_aux_var <- subsetGrid(grid_aux, var = var)\n",
    "      at.new <- seq(0,round(max(grid_aux_var[['Data']]), 2),round(max(grid_aux_var[['Data']]), 2)/20)\n",
    "      spatialPlot(grid_aux_var,\n",
    "                  backdrop.theme = \"coastline\",\n",
    "                  col.regions = cb.new,\n",
    "                  colorkey = list(draw = T, space = \"bottom\", width = .95,labels = list(cex = 1.4)),\n",
    "                  ylab = var,\n",
    "                  set.min = 0,\n",
    "                  set.max = at.new[length(at.new)],\n",
    "                  at = at.new,\n",
    "                  sp.layout = sp.layout.args,\n",
    "                  cex.labels = 1.4,\n",
    "                  names.attr = names.ts,\n",
    "                  layout = c(1,7),\n",
    "                  xlim =  c(-9.74, 4.5))\n",
    "      \n",
    "    }else if(var == \"va850\"){\n",
    "      \n",
    "      cb.new <- brewer.pal(n = 8, \"YlGn\")\n",
    "      cb.new <- c(\"#FFFFFF\", cb.new) %>% colorRampPalette()\n",
    "      grid_aux_var <- subsetGrid(grid_aux, var = var)\n",
    "      at.new <- seq(0,round(max(grid_aux_var[['Data']]), 2),round(max(grid_aux_var[['Data']]), 2)/20)\n",
    "      spatialPlot(grid_aux_var,\n",
    "                  backdrop.theme = \"coastline\",\n",
    "                  col.regions = cb.new,\n",
    "                  colorkey = list(draw = T, space = \"bottom\", width = .95,labels = list(cex = 1.4)),\n",
    "                  ylab = var,\n",
    "                  set.min = 0,\n",
    "                  set.max = at.new[length(at.new)],\n",
    "                  at = at.new,\n",
    "                  sp.layout = sp.layout.args,\n",
    "                  cex.labels = 1.4,\n",
    "                  names.attr = names.ts,\n",
    "                  layout = c(1,7),\n",
    "                  xlim =  c(-9.74, 4.5))\n",
    "      \n",
    "    }else if(var == \"ua850\"){\n",
    "      \n",
    "      cb.new <- brewer.pal(n = 8, \"YlGn\")\n",
    "      cb.new <- c(\"#FFFFFF\", cb.new) %>% colorRampPalette()\n",
    "      grid_aux_var <- subsetGrid(grid_aux, var = var)\n",
    "      at.new <- seq(0,round(max(grid_aux_var[['Data']]), 2),round(max(grid_aux_var[['Data']]), 2)/20)\n",
    "      spatialPlot(grid_aux_var,\n",
    "                  backdrop.theme = \"coastline\",\n",
    "                  col.regions = cb.new,\n",
    "                  colorkey = list(draw = T, space = \"bottom\", width = .95,labels = list(cex = 1.4)),\n",
    "                  ylab = var,\n",
    "                  set.min = 0,\n",
    "                  set.max = at.new[length(at.new)],\n",
    "                  at = at.new,\n",
    "                  sp.layout = sp.layout.args,\n",
    "                  cex.labels = 1.4,\n",
    "                  names.attr = names.ts,\n",
    "                  layout = c(1,7),\n",
    "                  xlim =  c(-9.74, 4.5))\n",
    "      \n",
    "    }else if(var == \"ta850\"){\n",
    "      \n",
    "      cb.new <- brewer.pal(n = 8, \"YlGn\")\n",
    "      cb.new <- c(\"#FFFFFF\", cb.new) %>% colorRampPalette()\n",
    "      grid_aux_var <- subsetGrid(grid_aux, var = var)\n",
    "      at.new <- seq(0,round(max(grid_aux_var[['Data']]), 2),round(max(grid_aux_var[['Data']], 2)/20)\n",
    "      spatialPlot(grid_aux_var,\n",
    "                  backdrop.theme = \"coastline\",\n",
    "                  col.regions = cb.new,\n",
    "                  colorkey = list(draw = T, space = \"bottom\", width = .95,labels = list(cex = 1.4)),\n",
    "                  ylab = var,\n",
    "                  set.min = 0,\n",
    "                  set.max = at.new[length(at.new)],\n",
    "                  at = at.new,\n",
    "                  sp.layout = sp.layout.args,\n",
    "                  cex.labels = 1.4,\n",
    "                  names.attr = names.ts,\n",
    "                  layout = c(1,7),\n",
    "                  xlim =  c(-9.74, 4.5))\n",
    "    }else{\n",
    "      \n",
    "      spatialPlot(subsetGrid(grid_aux, var = var),\n",
    "                  backdrop.theme = \"coastline\",\n",
    "                  col.regions = cb,\n",
    "                  colorkey = list(draw = T, space = \"bottom\", width = .95,labels = list(cex = 1.4)),\n",
    "                  ylab = var,\n",
    "                  set.min = 0,\n",
    "                  set.max = at[length(at)],\n",
    "                  at = at,\n",
    "                  sp.layout = sp.layout.args,\n",
    "                  cex.labels = 1.4,\n",
    "                  names.attr = names.ts,\n",
    "                  layout = c(1,7),\n",
    "                  xlim =  c(-9.74, 4.5))\n",
    "      \n",
    "    }\n",
    "  })\n",
    "  \n",
    "  figs.region[[length(figs.region)+1]] <- grid.arrange(grobs = c(list(figs_fwi[[1]], \n",
    "                                                                      figs_fwi[[3]],\n",
    "                                                                      figs_fwi[[5]])),\n",
    "                                                       ncol = 3, nrow = 1, as.table = FALSE, top = NULL)\n",
    "  \n",
    "  \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b2c619-86d1-40ba-b2b0-bb36d4fd1485",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf(\"./figures/lstm_xai_ATL_p90.pdf\", width = 15, height = 20)\n",
    "grid.arrange(figs.region[[2]])\n",
    "dev.off()\n",
    "\n",
    "pdf(\"./figures/lstm_xai_COASM_p90.pdf\", width = 15, height = 20)\n",
    "grid.arrange(figs.region[[1]])\n",
    "dev.off()\n",
    "\n",
    "pdf(\"./figures/lstm_xai_CONTM_p90.pdf\", width = 15, height = 20)\n",
    "grid.arrange(figs.region[[3]])\n",
    "dev.off()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
