{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d584b3c",
   "metadata": {},
   "source": [
    "# Learning the spatial structure of Fire Weather Index with convolutional neural networks\n",
    "\n",
    "***Environmental Research Letters***\n",
    "\n",
    "**O. Mirones, J. Baño-Medina, J. Bedia**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8bf861",
   "metadata": {},
   "source": [
    "This notebook reproduces the results presented in the manuscript entitled **Learning the spatial structure of Fire Weather Index with convolutional neural networks**, submitted to the journal *Environmental Research Letters* by *O. Mirones, J. Baño-Medina, J. Bedia*. \n",
    "This paper analyzes the ability of state-of-the-art machine learning algorithms, especially convolutional neural networks, to model the spatial structure of the Fire Weather Index (FWI)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b290bc8",
   "metadata": {},
   "source": [
    "## Table of contents:\n",
    "*  [1 Preparing the R environment](#1-bullet)\n",
    "*  [2  Load data](#2-bullet)\n",
    "    *  [2.1  Load predictor data](#2.1-bullet)\n",
    "    *  [2.2  Load predictand data](#2.2-bullet)\n",
    "*  [3  Downscaling: Cross-validation](#3-bullet)\n",
    "    *  [3.1  Analogs](#3.1-bullet)\n",
    "    *  [3.2  Generalized Linear Models](#3.2-bullet)\n",
    "    *  [3.3  Convolutional neural network multi-site](#3.3-bullet)\n",
    "    *  [3.4  Convolutional neural network multi-site gaussian](#3.4-bullet)\n",
    "    *  [3.5  Convolutional neural network multi-site multi-gaussian](#3.5-bullet)\n",
    "*  [4  Results](#4-bullet)\n",
    "    *  [4.1  Correlograms](#4.1-bullet)\n",
    "    *  [4.2  Mutual Information](#4.2-bullet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f137767d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> This notebook was run on a machine with the following technical specifications: !!!ESTÁ HECHO EN LUSTRE: PREGUNTAR A ANTONIO Y ZEQUI!!!\n",
    "\n",
    "- Operating system: Ubuntu 18.04.3 LTS (64 bits)\n",
    "- Memory: 60 GiB\n",
    "- Processor: 2x Intel(R) Xeon(R) CPU E5-2670 0 @ 2.60GHz (16 cores, 32 threads)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbb0b1d",
   "metadata": {},
   "source": [
    "## 1. Preparing the R environment <a class=\"anchor\" id=\"1-bullet\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155a4bc6",
   "metadata": {},
   "source": [
    "In particular, the following C4R libraries are used along the notebook:\n",
    "\n",
    "\n",
    " * `loadeR` (v1.7.0) for data loading,\n",
    " * `transformeR` (v2.1.0) for data manipulation, \n",
    " * [`downscaleR`](https://doi.org/10.5194/gmd-13-1711-2020) (v3.3.2) for downscaling and\n",
    " * [`downscaleR.keras`](https://doi.org/10.5194/gmd-13-2109-2020) (v1.0.0) for downscaling with neural networks and"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899dd6cc",
   "metadata": {},
   "source": [
    "A frozen version of the above libraries and all the ones needed to reproduce the manuscript are installable through the `environment.yml` file using the `mamba` package, by executing the following commands in your command shell terminal: \n",
    "```shell\n",
    "$ mamba env create -n deep-fwi --file=environment.yml\n",
    "```\n",
    "\n",
    "Once installed, you activate the newly created `deep-fwi` environment using `conda` by typing the following command:\n",
    "```shell\n",
    "$ conda activate deep-fwi\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae69a86f",
   "metadata": {},
   "source": [
    "Once in the environment, type `R` and load the libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4b7d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Loading libraries\n",
    "library(loadeR)\n",
    "library(transformeR)\n",
    "library(downscaleR)\n",
    "library(magrittr)\n",
    "library(downscaleR.keras)\n",
    "library(tfprobability)\n",
    "library(MASS)\n",
    "library(VALUE)\n",
    "library(geosphere)\n",
    "library(scales)\n",
    "library(abind)\n",
    "library(gridExtra)\n",
    "library(lattice)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7386f5",
   "metadata": {},
   "source": [
    "We load some auxiliary functions from the `utils` directory. The file `./utils/topologiesCNN.R` contains the code implementing the convolutional neural networks used in this study, which are based on tensorflow-keras. The file `./utils/downscaleCV.keras.tfprobability.R` is a function enabling cross-validation over a convolutional neural network coded with the library tensorflow-probability. The file `./utils/dimFix.R` completes missing dimensions of VALUE objects. The file `./utils/corrMat.VALUE.R`computes the cross correlation matrices between stations that serve as input for plotting functions. These two files are a variation of its namesake from the package `VALUE`, adapted for use with notebook data structures. The file `./utils/corrL_allST.R` returns the correlation length for the `corrMat.VALUE` output. The `corrL_allST` function uses the file `./utils/corr_length.R` to calculate the correlation length based on the smoothed fit to the given data. Here, the `./utils/smooth_corr.R` smooths using a loess filter with standard settings. Finally, the file `./utils/miMat.VALUE.R`computes the mutual information between stations. These files is a variation of ist namesake from the package `VALUE`, adapted for the Fire Weather Index (`FWI`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c31fd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "source(\"./utils/topologiesCNN.R\")\n",
    "source(\"./utils/downscaleCV.keras.tfprobability.R\")\n",
    "source(\"./utils/dimFix.R\")\n",
    "source(\"./utils/corrMat.VALUE.R\")\n",
    "source(\"./utils/corrL_allST.R\")\n",
    "source(\"./utils/corr_length.R\")\n",
    "source(\"./utils/smooth_corr.R\")\n",
    "source(\"./utils/miMat.VALUE.R\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d8d6ea",
   "metadata": {},
   "source": [
    "Below we define parameters related to the cross-validation procedure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd4a1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "folds <- list(1985:1991, 1992:1998, 1999:2004, 2005:2011)\n",
    "vars <- c(\"hur850\", \"ta850\", \"tas\", \"ua850\", \"va850\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f445afb4",
   "metadata": {},
   "source": [
    "## 2 Load data <a class=\"anchor\" id=\"2-bullet\"></a>\n",
    "In this section we illustrate how to load the data into our `R` environment. Previously, we need to download the data from the Zenodo repository (DOI:) and store it in our working directory. For the notebook to work succesfully, it is important to preserve the structure of directories encountered in Zenodo. In addition we create a new folder where we will store the estimated fields named `./data/predictions/`. The folder `figures` is created to store the figures of the results obtained. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd790679",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Store in \"./data/\" the predictor and predictand files from Zenodo (DOI: ) such that you end up having \"./data/predictors/\", \"./data/predictands/\" and \"./figures/\" \n",
    "if (!dir.exists(\"./data/\")) dir.create(\"./data/\", showWarnings = FALSE)\n",
    "###\n",
    "if (!dir.exists(\"./data/predictions/\")) dir.create(\"./data/predictions/\", showWarnings = FALSE)\n",
    "###\n",
    "if (!dir.exists(\"./figures/\")) dir.create(\"./figures/\", showWarnings = FALSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06211928",
   "metadata": {},
   "source": [
    "## 2.1 Load predictor data <a class=\"anchor\" id=\"2.1-bullet\"></a>\n",
    "We load the following predictor variables: relative humidity at 850hPa (`hur850`), air temperature at 850 hPa (`ta850`), air surface temperature (`tas`), zonal wind velocity at 850 hPa (`ua850`), and meridional wind velocity at 850 hPa (`va850`). These variables are stored as `.nc` files in the directory `./data/predictors/`. We use `loadGridData` from library `loadeR` to load the data into our `R` session. Then we call function `makeMultiGrid` to bind these predictor variables into a single object named `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c0a693",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Loading predictor data\n",
    "x <- lapply(vars, FUN = function(var) {\n",
    "  out <- loadGridData(paste0(\"./data/predictors/\", var, \".nc\"), var = var)\n",
    "  print(sprintf(\"Variable: %s loaded!\", var))\n",
    "  return(out)\n",
    "}) %>% makeMultiGrid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f6cfab",
   "metadata": {},
   "source": [
    "## 2.2 Load predictand data <a class=\"anchor\" id=\"2.1-bullet\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad965d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Loading predictand data (previously generated in 1_loadObservations.R)\n",
    "y <- load(\"./Rdata/AEMET_SUBSET/AEMET_13UTC_fwi13.Rdata\") %>% get() %>% subsetGrid(years = 1985:2011)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592668cc",
   "metadata": {},
   "source": [
    "## 3 Downscaling: Cross-validation <a class=\"anchor\" id=\"3-bullet\"></a>\n",
    "In this section we perform cross-validation over the machine learning methods used in this study: analogs, generalized linear models, and 3 different convolutional neural network architectures. Therefore, we split the data into the following 4 chronological folds: 1985-1991, 1992-1998, 1999-2004, 2005-2011. We store the resulting predictions into the previously created `./data/predictions/` folder. These files will be used in the next section. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa3fd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.dir <- \"./data/predictions/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b10e89",
   "metadata": {},
   "source": [
    "The results presented in this section are predominantly based on libraries `downscaleR`, `transformeR` and `downscaleR.keras`. In particular we use the function `downscaleCV` from library `downscaleR` to perform cross-validation for the analogs and generalized linear models. Similary, we use the function `downscaleCV.keras` from `downscaleR.keras` to cross-validate the convolutional neural networks. The only exception is the convolutional model multi-site multi-gaussian which builds on function `downscaleCV.keras.tfprobability` located in the `utils` directory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba00095",
   "metadata": {},
   "source": [
    "## 3.1 Analogs <a class=\"anchor\" id=\"3.1-bullet\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d656756",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Downscaling analogs\n",
    "p <- downscaleCV(x = x,\n",
    "                 y = y,\n",
    "                 method = \"analogs\",\n",
    "                 n.analogs = number_analogs,\n",
    "                 sel.fun = \"mean\",\n",
    "                 window = 7,\n",
    "                 scaleGrid.args = list(type = \"standardize\"),\n",
    "                 folds = folds)\n",
    "predFileName <- sprintf(\"%spred_analogs_AEMET_13UTC_fwi13.Rdata\", pred.dir)\n",
    "save(p, file = predFileName)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb1feb3",
   "metadata": {},
   "source": [
    "## 3.2 Generalized Linear Model <a class=\"anchor\" id=\"3.2-bullet\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad06d60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Downscaling generalized linear model\n",
    "number_of_neighbours = 16\n",
    "p <- downscaleCV(x = x,\n",
    "                 y = y,\n",
    "                 method = \"GLM\",\n",
    "                 family = \"gaussian\",\n",
    "                 prepareData.args = list(\"local.predictors\" = list(n = number_of_neighbours, vars = getVarNames(x))),\n",
    "                 scaleGrid.args = list(type = \"standardize\"),\n",
    "                 folds = folds)\n",
    "predFileName <- sprintf(\"%spred_glm%s_AEMET_13UTC_fwi13.Rdata\", pred.dir, number_of_neighbours)\n",
    "save(p, file = predFileName)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368d52ee",
   "metadata": {},
   "source": [
    "## 3.3 Convolutional neural network multi-site <a class=\"anchor\" id=\"3.3-bullet\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e6ad47",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Downscaling convolutional neural network multi-site\n",
    "model <- cnn_model(topology = \"cnn-multi-site\",\n",
    "                   input_shape = c(getShape(x, \"lat\"), getShape(x, \"lon\"), getShape(x, \"var\")),\n",
    "                   output_shape = getShape(y, \"loc\"),\n",
    "                   kernel_size = c(3,3),\n",
    "                   neurons = c(50, 50))\n",
    "p <- downscaleCV.keras(x = x,\n",
    "                       y = y,\n",
    "                       model = model,\n",
    "                       prepareData.keras.args = list(first.connection = \"conv\", last.connection = \"dense\", channels = \"last\"),\n",
    "                       compile.args = list(\"loss\" = \"mse\", \"optimizer\" = optimizer_adam(lr = 0.0001)),\n",
    "                       fit.args = list(\"batch_size\" = 100, \"epochs\" = 10000, \"validation_split\" = 0.1, \"verbose\" = 0, \"callbacks\" = list(callback_early_stopping(patience = 30))),\n",
    "                       scaleGrid.args = list(type = \"standardize\"),\n",
    "                       folds = folds)\n",
    "predFileName <- sprintf(\"%spred_cnn-multi-site_AEMET_13UTC_fwi13.Rdata\", pred.dir)\n",
    "save(p, file = predFileName)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb23ece",
   "metadata": {},
   "source": [
    "## 3.4 Convolutional neural network multi-site gaussian <a class=\"anchor\" id=\"3.4-bullet\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4ba7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Downscaling convolutional neural network multi-site gaussian\n",
    "model <- cnn_model(topology = \"cnn-multi-site-gaussian\",\n",
    "                   input_shape = c(getShape(x, \"lat\"), getShape(x, \"lon\"), getShape(x, \"var\")),\n",
    "                   output_shape = getShape(y, \"loc\"),\n",
    "                   kernel_size = c(3,3),\n",
    "                   neurons = c(50, 50))\n",
    "p <- downscaleCV.keras(x = x,\n",
    "                       y = y,\n",
    "                       model = model,\n",
    "                       loss = \"gaussianLoss\",\n",
    "                       prepareData.keras.args = list(first.connection = \"conv\", last.connection = \"dense\", channels = \"last\"),\n",
    "                       compile.args = list(\"loss\" = gaussianLoss(last.connection = \"dense\"), \"optimizer\" = optimizer_adam(lr = 0.0001)),\n",
    "                       fit.args = list(\"batch_size\" = 100, \"epochs\" = 10000, \"validation_split\" = 0.1, \"verbose\" = 0, \"callbacks\" = list(callback_early_stopping(patience = 30))),\n",
    "                       scaleGrid.args = list(type = \"standardize\"),\n",
    "                       folds = folds)\n",
    "p <- lapply(1:30, FUN = function(member) {\n",
    "  computeTemperature(mean = subsetGrid(p, var = \"mean\"), log_var = subsetGrid(p, var = \"log_var\"))\n",
    "}) %>% bindGrid(dimension = \"member\")\n",
    "attr(p$Data, \"dimensions\") <- c(\"member\",\"time\", \"loc\")\n",
    "p <- binaryGrid(p, condition = \"GT\", threshold = 0, partial = TRUE)\n",
    "predFileName <- sprintf(\"./Rdata/%s/pred_cnn-multi-site-gaussian_30samples_AEMET_13UTC_fwi13.Rdata\", dataset)\n",
    "save(p, file = predFileName)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232d5ff7",
   "metadata": {},
   "source": [
    "## 3.5 Convolutional neural network multi-site multi-gaussian <a class=\"anchor\" id=\"3.5-bullet\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e12872",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Downscaling convolutional neural network multi-site multi-gaussian\n",
    "negloglik <- function (x, rv_x) - (rv_x %>% tfd_log_prob(x))\n",
    "p <- downscaleCV.keras.tfprobability(x = x,\n",
    "                                     y = y,\n",
    "                                     samples = 30,\n",
    "                                     cnn_model.args = list(topology = \"cnn-multi-site-multi-gaussian\",\n",
    "                                                           input_shape = c(getShape(x, \"lat\"), getShape(x, \"lon\"), getShape(x, \"var\")),\n",
    "                                                           output_shape = getShape(y, \"loc\"),\n",
    "                                                           kernel_size = c(3,3),\n",
    "                                                           neurons = c(200, 200)),\n",
    "                                     type_nn = \"multivariate-gaussian\",\n",
    "                                     prepareData.keras.args = list(first.connection = \"conv\", last.connection = \"dense\", channels = \"last\"),\n",
    "                                     compile.args = list(\"loss\" = negloglik, \"optimizer\" = optimizer_adam(lr = 0.0001)),\n",
    "                                     fit.args = list(\"batch_size\" = 100L, \"epochs\" = 10000L, \"validation_split\" = 0.1, \"verbose\" = 0L, \"callbacks\" = list(callback_early_stopping(patience = 30))),\n",
    "                                     scaleGrid.args = list(type = \"standardize\"),\n",
    "                                     folds = folds)\n",
    "predFileName <- sprintf(\"./Rdata/%s/pred_cnn-multi-site-multi-gaussian_30samples_AEMET_13UTC_fwi13.Rdata\", dataset)\n",
    "save(p, file = predFileName)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb57fa1c",
   "metadata": {},
   "source": [
    "## 4 Results <a class=\"anchor\" id=\"4-bullet\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd677e43",
   "metadata": {},
   "source": [
    "In this section, we will present an analysis of the results obtained from the different predictions. Firstly, we will display the correlograms that have been constructed for each prediction. Following that, we will present the mutual information matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87e1e18",
   "metadata": {},
   "source": [
    "## 4.1 Correlograms <a class=\"anchor\" id=\"4.1-bullet\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667a53cb",
   "metadata": {},
   "source": [
    "Here, we assess the strength of linear relationships between observed and predicted `FWI` time series among different locations. We accomplish this by calculating pairwise *Spearman's* cross-correlations among all pairs of stations. To visualize these relationships, we construct a location plot by plotting the cross-correlation value for each station pair against their respective geographical distances.\n",
    "\n",
    "Next, we fit a local 2nd-order polynomial (loess) to obtain two curves—one for predictions and one for observations. This enables a straightforward visual assessment of the spatial correlation structure of predictions compared to the reference observations.\n",
    "\n",
    "For the analysis, we define the fire season as June to September (JJAS), and we set a threshold ($0.4$) for the correlation length and specify the correlation type. The correlations are then computed using the `corrL_allST` function.\n",
    "\n",
    "Additionally, we calculate the Root Mean Square Error (`RMSE`), Mean Absolute Error (`MAE`), and the correlation length bias (`CL bias`). These metrics provide valuable insights into the performance of the predictions compared to the reference observations. All the computed values, including correlations, correlation length, `RMSE`, `MAE`, and `CL bias`, are included in the correlation plots.\n",
    "\n",
    "To facilitate easy access and reference, the resulting figure will be stored in the `figures` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c45f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Defining season, threshold and correlation type\n",
    "\n",
    "sea <- \"JJAS\"\n",
    "tr <- .4\n",
    "corrtype <- \"spearman\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef53c89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Computing and plotting the correlogram for the reference observations\n",
    "pdf(\"./figures/correlograms.pdf\", width = 14, height = 8.5)\n",
    "par(mfrow = c(2,3))\n",
    "\n",
    "corrL_obs <- corrL_allST(data=y, type = \"after\", corrtype = corrtype,tr=tr,season=sea)\n",
    "\n",
    "plot(corrL_obs$JJAS, col = alpha('grey', 0.3), pch = 16, ylim = c(-0.3,1),\n",
    "     xlab = \"Distance [km]\", ylab = \"Correlation\", main = \"Obs\", cex.lab = 1.5, cex.main = 1.5, cex.axis = 1.5,\n",
    "     las = 1)\n",
    "abline(h = tr, lty = 2)\n",
    "abline(v = corrL_obs$JJA$corrL, lwd = 2, col = 'grey')\n",
    "lines(corrL_obs$JJAS$x_fit, corrL_obs$JJAS$y_fit, col = 'grey', lwd = 2.5)\n",
    "grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be782b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Computing and plotting the correlograms for the predictions\n",
    "preds <- list.files(pred.dir)\n",
    "\n",
    "for (i in c(1:length(preds))) {\n",
    "  \n",
    "  ###Loading prediction and computing the correlation length\n",
    "  \n",
    "  p <- load(paste0(pred.dir, preds[i]))%>% get()\n",
    "  corrL_pred <- corrL_allST(data=p, type = \"after\", corrtype = corrtype,tr=tr,season=sea)\n",
    "  \n",
    "  ###Computing RMSE and MAE\n",
    "  \n",
    "  d <- corrL_obs$JJAS$y - corrL_pred$JJAS$y\n",
    "  mae <- mean(abs(d))\n",
    "  mse <- mean((d)^2)\n",
    "  rmse <- sqrt(mse)\n",
    "  bias.corrL <- corrL_obs$JJAS$corrL-corrL_pred$JJAS$corrL\n",
    "  \n",
    "  ###Visualization\n",
    "  plot(corrL_pred$JJAS, col = alpha('red', 0.3), pch = 16, ylim = c(-0.3,1),\n",
    "       xlab = \"Distance [km]\", ylab = \"Correlation\",\n",
    "       main = gsub(\"^pred_(.*?)_AEMET.*$\", \"\\\\U\\\\1\", preds[i], perl = TRUE), \n",
    "       cex.lab = 1.5, cex.main = 1.5, cex.axis = 1.5, las = 1)\n",
    "  abline(h = tr, lty = 2)\n",
    "  abline(v = corrL_obs$JJAS$corrL, lwd = 2, col = 'grey')\n",
    "  lines(corrL_obs$JJAS$x_fit, corrL_obs$JJAS$y_fit, col = 'grey', lwd = 2.5) \n",
    "  abline(v = corrL_pred$JJAS$corrL, lwd = 2, col = 'red')\n",
    "  lines(corrL_pred$JJAS$x_fit, corrL_pred$JJAS$y_fit, col = 'red', lwd = 2.5)\n",
    "  text(1225,0.85, paste0(\"RMSE: \", round(rmse,3)), cex = 1.45)\n",
    "  text(1225,0.75, paste0(\"MAE: \", round(mae,3)), cex = 1.45)\n",
    "  text(1225,0.95, paste0(\"CL bias: \", round(bias.corrL, 3)), cex = 1.45)\n",
    "  grid()\n",
    " \n",
    "}\n",
    "\n",
    "dev.off()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8c5998",
   "metadata": {},
   "source": [
    "## 4.2 Mutual Information <a class=\"anchor\" id=\"4.2-bullet\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f905965",
   "metadata": {},
   "source": [
    "We proceed to plot each mutual information (`MI`) value, denoted as $M_{i,j}$, against the distance between the corresponding locations, i.e., $i$ and $j$. To capture the underlying patterns, we fit a second-degree loess curve to these plots.\n",
    "\n",
    "To determine the mutual information lengths (`MIL`), we establish MI thresholds for both the observations and the different downscaling methods. In this analysis, we utilize a MI threshold of $0.05$. This threshold allows for comparable results to correlation length analyses and aids in the identification of potential new information regarding the performance of each method.\n",
    "\n",
    "Based on the climatic regions, the stations are grouped together in a matrix. This matrix exhibits three primary groupings, each with a quadrilateral shape. These groupings correspond to the following climatic regions: the *Atlantic* region, the *Continental Mediterranean* region, and the *Coastal Mediterranean* stations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1910ba44",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Defining the order of the stations\n",
    "\n",
    "st.names <- c(\"REUS\",\"S.COMPOSTELA\",\"VIGO\",\"SORIA\",\"VALLADOLID\",\"ZAMORA\",\"LEON\",\"SALAMANCA\",\n",
    "               \"MADRID-BARAJAS\",\"MADRID-RETIRO\",\"CIUDAD REAL\",\"BADAJOZ\",\"GRANADA\",\n",
    "               \"SEVILLA\",\"MORON\",\"JEREZ\",\"ALMERIA\",\"MURCIA\",\"ALICANTE-ALTET\",\n",
    "               \"ALICANTE\",\"CUENCA\",\"VALENCIA-AER.\",\"VALENCIA\",\"LOGRONO\",\"DAROCA\",\"TORTOSA\",\n",
    "               \"MALLORCA\",\"MENORCA\",\"IBIZA\")\n",
    "\n",
    "y$Metadata$name <- st.names\n",
    "\n",
    "###Defining the FWI90 event for the mutual information\n",
    "\n",
    "prob <- .9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10d09ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "###defining the color palettes\n",
    "\n",
    "mi.colors <- RColorBrewer::brewer.pal(11, \"Reds\") %>% colorRampPalette()\n",
    "spec.colors <- RColorBrewer::brewer.pal(11, \"Spectral\") %>% colorRampPalette()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c2852c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#modification of stations order\n",
    "ind_station.ids <- c(2:10,21,24,13,12,11,25,1,27:29,18:20,22,23,26,14:17)\n",
    "station.ids <- y$Metadata$station_id[ind_station.ids]\n",
    "y <- subsetGrid(y, station.id = station.ids) %>% redim(member = FALSE, loc = TRUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6916894c",
   "metadata": {},
   "source": [
    "In this step, we compute the `MI` matrix for both the reference observations and the predictions. To perform these computations, we utilize the `miMat.VALUE` function. This function enables us to calculate the `MI` values and construct the `MI` matrix, which represents the pairwise `MI` between different locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e577fbfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Mutual Information matrix computation for the observations\n",
    "\n",
    "mi.matrix.obs <- miMat.VALUE(y,\n",
    "                         predictionObj = NULL,\n",
    "                         season = \"JJAS\",\n",
    "                         threshold = NULL,\n",
    "                         prob = prob)\n",
    "\n",
    "\n",
    "idx <- grep(\"analogs\", preds, invert = T)\n",
    "preds <- preds[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37cb66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Mutual Information matrix computation for the predictions\n",
    "\n",
    "mi.matrix.list <- list()\n",
    "for (i in c(1:length(preds))) {\n",
    "\n",
    "  ### Loading predictions\n",
    "  \n",
    "  p <- load(paste0(pred.dir, preds[i]))%>% get()\n",
    "  \n",
    "  ###Ordering stations by location\n",
    "  \n",
    "  p <- subsetGrid(p, station.id = station.ids) %>% redim(member = FALSE, loc = TRUE)\n",
    "  \n",
    "  ###Computing the Mutual Information matrix\n",
    "  \n",
    "  mi.matrix <- miMat.VALUE(p,\n",
    "                predictionObj = NULL,\n",
    "                season = \"JJAS\",\n",
    "                threshold = NULL,\n",
    "                prob = prob)\n",
    "  \n",
    "  mi.matrix.list[[i]] <- mi.matrix\n",
    "  \n",
    "  \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3ca4f9",
   "metadata": {},
   "source": [
    "Lastly, we proceed to plot the `MI` matrices for the `FWI90` event during the fire season (June to September). The first matrix computed represents the MI for the reference observations, displayed in the upper triangle, and the best-performing model (`CNN-MSMG`) in the lower triangle. The bottom left and right matrices depict the `MI` bias of the methods compared to the observations.\n",
    "\n",
    "In the `MI` bias matrices, the analogs method is excluded from the representation since `MI` biases are negligible by construction. These matrices are divided to show the bias corresponding to the different methods in the upper and lower triangles. Only pairs of stations with `MI` values $\\ge 0.05$ for the observations are displayed in the `MI` bias matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0488ab37",
   "metadata": {},
   "outputs": [],
   "source": [
    "scales.list <- list(x = list(labels = attributes(mi.matrix.obs[[1]])$station_names, rot = 90,\n",
    "                             at = seq(1,ncol(mi.matrix.obs[[1]]),1), cex = .65),\n",
    "                    y = list(labels = attributes(mi.matrix.obs[[1]])$station_names,\n",
    "                             at = seq(1,ncol(mi.matrix.obs[[1]]),1), cex = .65),\n",
    "                   alternating = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7a1e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Computing the first matrix: the upper diagonal contains the mutual information for the obs;\n",
    "###the lower diagonal is for the cnn-multi-site-multi-gaussian\n",
    "\n",
    "new.mi.matrix <- mi.matrix.obs\n",
    "\n",
    "idx.lower.tri <- which(lower.tri(new.mi.matrix$JJAS), arr.ind = T)\n",
    "\n",
    "new.mi.matrix$JJAS[idx.lower.tri] <- mi.matrix.list[[grep(\"cnn-multi-site-multi-gaussian\", preds)]]$JJAS[idx.lower.tri] #changing the lower triangle\n",
    "\n",
    "l1 <- levelplot(new.mi.matrix$JJAS, ylab = \"AEMET_13UTC_FWI13 (Obs)\" ,\n",
    "                xlab = \"CNN-MULTI-SITE-MULTI-GAUSSIAN\",\n",
    "                scales = scales.list, main = \"\", col.regions = mi.colors(100),\n",
    "               colorkey = list(space = \"bottom\", height = 1, width = 1)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca4849b",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Replacing with NAs those values with MI < 0.05 (below the threshold defined)\n",
    "\n",
    "idx.na <- which(mi.matrix.obs$JJAS < 0.05, arr.ind = T)\n",
    "\n",
    "\n",
    "names <- gsub(\"^pred_(.*?)_AEMET.*$\", \"\\\\U\\\\1\", preds, perl = TRUE)\n",
    "\n",
    "levelplot.list <- list()\n",
    "for (i in c(1,3)) {\n",
    "  \n",
    "  new.mi.matrix <- mi.matrix.list[[i]]\n",
    "  idx.lower.tri <- which(lower.tri(new.mi.matrix$JJAS), arr.ind = T)\n",
    "  new.mi.matrix$JJAS[idx.lower.tri] <- mi.matrix.list[[i+1]]$JJAS[idx.lower.tri]\n",
    "  \n",
    "  new.mi.matrix$JJAS[idx.na] <- NA\n",
    "  \n",
    "  l <- levelplot(new.mi.matrix$JJAS-mi.matrix.obs$JJAS, ylab = names[i], xlab = names[i+1], scales = scales.list,\n",
    "                 main = \"\", col.regions = rev(spec.colors(100)), at = seq(-0.15,0.15,0.01),\n",
    "                 colorkey = list(space = \"bottom\", height = 1, width = 1)) \n",
    "  \n",
    "  levelplot.list[[length(levelplot.list)+1]] <- l\n",
    "}\n",
    "\n",
    "pdf(\"./figures/MI_Matrices.pdf\", width = 22.5, height = 11)\n",
    "\n",
    "grid.arrange(l1, levelplot.list[[1]], levelplot.list[[2]], nrow = 2, ncol = 2)\n",
    "\n",
    "dev.off()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
